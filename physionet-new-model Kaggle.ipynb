{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261b9f78-0d7b-43ec-b9bf-1919f7af025e",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 1 â€” Install Required Kaggle Packages\n",
    "\n",
    "This removes TensorFlow (not needed) and installs a dependency required by the Hengck23 pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5791974-1d60-48ee-b4b9-5e604de03d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y tensorflow\n",
    "!uv pip install --no-deps --system --no-index --find-links='/kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet/setup' connected-components-3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb270dc9-0e3e-4d35-88d8-c4947419fadd",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 2 â€” Import Libraries & Competition Modules\n",
    "\n",
    "Loads PyTorch, CV tools, signal processing, and the 3-stage ECG digitization framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cb99a-13a4-4c4c-b243-350b17935caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, cv2, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "sys.path.append('/kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet')\n",
    "import stage0_common as s0c\n",
    "import stage1_common as s1c\n",
    "import stage2_common as s2c\n",
    "from stage0_model import Net as Stage0Net\n",
    "from stage1_model import Net as Stage1Net\n",
    "from stage2_model import MyCoordUnetDecoder, encode_with_resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c37fae-3f91-4a21-818f-d0b407128a4d",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 3 â€” ECG Lead Order Definition\n",
    "\n",
    "Defines the standard 12-lead order for final submission formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab1354-cc72-47a3-a91d-06781a7eef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEADS_ORDER = [\"I\",\"II\",\"III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ab0b8-6fd7-43dc-927b-516a90e52544",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 4 â€” Image Color & Enhancement Utilities\n",
    "\n",
    "Functions for denoising, CLAHE enhancement, white balance, and illumination correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3615b-5415-4618-aaf1-cb530394df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_by_xcorr(y, ref_len):\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    t = np.linspace(0, 1, len(y))\n",
    "    ref = np.sin(2 * np.pi * t)  # dummy reference\n",
    "    corr = np.correlate(y - y.mean(), ref - ref.mean(), mode=\"full\")\n",
    "    shift = corr.argmax() - len(ref) + 1\n",
    "    return np.roll(y, -shift)\n",
    "\n",
    "def change_color(image_rgb):\n",
    "    hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v_denoised = cv2.fastNlMeansDenoising(v, h=5.46)\n",
    "    std = np.std(v_denoised)\n",
    "    clip_limit = max(1.0, min(3.5, 2.0 + std / 25))\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(8, 8))\n",
    "    v_enhanced = clahe.apply(v_denoised)\n",
    "    hsv_enhanced = cv2.merge([h, s, v_enhanced])\n",
    "    return cv2.cvtColor(hsv_enhanced, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def series_dict(series_4row):\n",
    "    series_4row = np.asarray(series_4row)\n",
    "    if series_4row.ndim == 3: series_4row = series_4row[0]\n",
    "    if series_4row.shape[0] != 4 and series_4row.shape[1] == 4: series_4row = series_4row.T\n",
    "\n",
    "    d = {}\n",
    "    names = [\n",
    "        ['I','aVR','V1','V4'],\n",
    "        ['II_short','aVL','V2','V5'],   \n",
    "        ['III','aVF','V3','V6'],\n",
    "    ]\n",
    "    for r in range(3):\n",
    "        for lead, arr in zip(names[r], np.array_split(series_4row[r], 4)):\n",
    "            d[lead] = np.asarray(arr, dtype=np.float32)\n",
    "\n",
    "    d['II'] = np.asarray(series_4row[3], dtype=np.float32)  # full 10s II\n",
    "    return d\n",
    "\n",
    "def clahe_luminance_bgr(img_bgr, clip=2.0, tile=8):\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=float(clip), tileGridSize=(int(tile), int(tile)))\n",
    "    l2 = clahe.apply(l)\n",
    "    return cv2.cvtColor(cv2.merge([l2, a, b]), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def grayworld_white_balance(img_bgr):\n",
    "    img = img_bgr.astype(np.float32)\n",
    "    b, g, r = cv2.split(img)\n",
    "    mb, mg, mr = b.mean(), g.mean(), r.mean()\n",
    "    m = (mb + mg + mr) / 3.0\n",
    "    b *= (m / (mb + 1e-6)); g *= (m / (mg + 1e-6)); r *= (m / (mr + 1e-6))\n",
    "    return np.clip(cv2.merge([b, g, r]), 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ccb16-96b5-4dc6-a1ce-7066355c46d1",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 5 â€” Einthoven correction on SHORT leads only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c6d29-ff9e-4015-a459-b7575a317fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dw(d, alpha=0.45):\n",
    "    if all(k in d for k in ['I','II_short','III']):\n",
    "        L1, L2s, L3 = d['I'], d['II_short'], d['III']\n",
    "        e = L2s - (L1 + L3)\n",
    "        d['I']        = L1 + alpha*e\n",
    "        d['III']      = L3 + alpha*e\n",
    "        d['II_short'] = L2s - alpha*e\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87963b-85bf-4da0-8e47-838726786e28",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 6 â€” Noise Reduction & Background Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ed2dd-8cb0-41e3-b8ce-d5bf6322fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_median(img_bgr, k=3):\n",
    "    k = int(k); k = k if k % 2 == 1 else k + 1\n",
    "    return cv2.medianBlur(img_bgr, k)\n",
    "\n",
    "def denoise_bilateral(img_bgr, d=7, sigmaColor=50, sigmaSpace=50):\n",
    "    return cv2.bilateralFilter(img_bgr, d=int(d), sigmaColor=float(sigmaColor), sigmaSpace=float(sigmaSpace))\n",
    "\n",
    "def illumination_strength(img_bgr, sigma=35):\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "    blur = cv2.GaussianBlur(gray, (0, 0), sigma)\n",
    "    return float(np.std(blur))\n",
    "\n",
    "def bg_correct_lab_l(img_bgr, k=81):\n",
    "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    k = int(k); k = k if k % 2 == 1 else k + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "    bg = cv2.morphologyEx(l, cv2.MORPH_OPEN, kernel)\n",
    "    l_corr = cv2.subtract(l, bg)\n",
    "    l_corr = cv2.normalize(l_corr, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return cv2.cvtColor(cv2.merge([l_corr, a, b]), cv2.COLOR_LAB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d10448-82ef-4e0c-b377-b1f4fe710ab7",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 7 â€” Source-Specific Preprocessing\n",
    "\n",
    "Applies different enhancement strategies depending on ECG source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fceb0df-3bcd-4d50-9200-b7738a3fb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_by_source(img_bgr, source):\n",
    "    s = str(source)\n",
    "    if s == \"0001\": return img_bgr\n",
    "    if s == \"0003\": return clahe_luminance_bgr(grayworld_white_balance(img_bgr), clip=1.2, tile=8)\n",
    "    if s == \"0004\": return img_bgr\n",
    "    if s == \"0006\":\n",
    "        x = denoise_bilateral(img_bgr, d=5, sigmaColor=25, sigmaSpace=25)\n",
    "        return clahe_luminance_bgr(x, clip=1.2, tile=8)\n",
    "    if s == \"0005\":\n",
    "        x = img_bgr\n",
    "        if illumination_strength(x, sigma=35) > 0.14: x = bg_correct_lab_l(x, k=81)\n",
    "        if cv2.cvtColor(x, cv2.COLOR_BGR2GRAY).std() < 30: x = clahe_luminance_bgr(x, clip=1.1, tile=8)\n",
    "        return x\n",
    "    if s == \"0009\":\n",
    "        x = img_bgr\n",
    "        if illumination_strength(x, sigma=35) > 0.14: x = bg_correct_lab_l(x, k=101)\n",
    "        return denoise_median(x, k=3)\n",
    "    if s == \"0010\":\n",
    "        x = img_bgr\n",
    "        if illumination_strength(x, sigma=35) > 0.14: x = bg_correct_lab_l(x, k=81)\n",
    "        if cv2.cvtColor(x, cv2.COLOR_BGR2GRAY).std() < 30: x = clahe_luminance_bgr(x, clip=1.15, tile=8)\n",
    "        return x\n",
    "    if s == \"0011\": return clahe_luminance_bgr(grayworld_white_balance(img_bgr), clip=1.2, tile=8)\n",
    "    if s == \"0012\": return img_bgr\n",
    "    return img_bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb373c6f-17cb-496c-aede-ff96081a37b6",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 8 â€” Stage-1 Quality Estimator\n",
    "\n",
    "Helps choose the best rectified ECG image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3b0969-3ae2-470d-97fd-ba52879d7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage1_quality(s1_rgb):\n",
    "    g = cv2.cvtColor(s1_rgb.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    e = cv2.Canny(g, 50, 150)\n",
    "    density = e.mean() / 255.0\n",
    "    gx = cv2.Sobel(g, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(g, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    ax = float(np.mean(np.abs(gx))); ay = float(np.mean(np.abs(gy)))\n",
    "    anis = max(ax, ay) / (min(ax, ay) + 1e-6)\n",
    "    return float(density * 0.7 + np.tanh(anis - 1.0) * 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5303f1-0f12-42c0-8a21-23c63be377cd",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 9 â€” Stage-2 Segmentation Network\n",
    "\n",
    "Defines UNet-style decoder for extracting ECG traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd25d4-f387-4049-815c-1d471e242fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        encoder_dim = [64, 128, 256, 512]\n",
    "        decoder_dim = [128, 64, 32, 16]\n",
    "        self.encoder = timm.create_model(\n",
    "            model_name='resnet34.a3_in1k',\n",
    "            pretrained=pretrained,\n",
    "            in_chans=3,\n",
    "            num_classes=0,\n",
    "            global_pool=''\n",
    "        )\n",
    "        self.decoder = MyCoordUnetDecoder(\n",
    "            in_channel=encoder_dim[-1],\n",
    "            skip_channel=encoder_dim[:-1][::-1] + [0],\n",
    "            out_channel=decoder_dim,\n",
    "            scale=[2, 2, 2, 2]\n",
    "        )\n",
    "        self.pixel = nn.Conv2d(decoder_dim[-1], 4, 1)\n",
    "        \n",
    "        def forward(self, image):\n",
    "        encode = encode_with_resnet(self.encoder, image)\n",
    "        last, _ = self.decoder(feature=encode[-1], skip=encode[:-1][::-1] + [None])\n",
    "        return self.pixel(last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cb7ad-9435-4567-9877-2f56f0213365",
   "metadata": {},
   "source": [
    "ðŸ”¹ SEGMENT 10 â€” Main 3-Stage ECG Pipeline Class\n",
    "\n",
    "Handles:\n",
    "â€¢ Stage-0 (rotation correction)\n",
    "â€¢ Stage-1 (grid rectification)\n",
    "â€¢ Stage-2 (signal extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff6b0d-79d5-477a-aa77-41ce412fb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysioPipeline:\n",
    "    def __init__(self, device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        self.stage0_net = self.stage1_net = self.stage2_net = None\n",
    "        self.x0, self.x1 = 0, 2176\n",
    "        self.y0, self.y1 = 0, 1696\n",
    "        self.zero_mv = [703.5, 987.5, 1271.5, 1531.5]\n",
    "        self.mv_to_pixel = 78.8\n",
    "        self.t0, self.t1 = 235, 4161\n",
    "        self.resize = T.Resize((1696, 4352), interpolation=T.InterpolationMode.BILINEAR)\n",
    "        \n",
    "    def load_models(self, stage0_w, stage1_w, stage2_w):\n",
    "        self.stage0_net = s0c.load_net(Stage0Net(pretrained=False), stage0_w).to(self.device).eval()\n",
    "        self.stage1_net = s1c.load_net(Stage1Net(pretrained=False), stage1_w).to(self.device).eval()\n",
    "        self.stage2_net = Net3(pretrained=False).to(self.device).eval()\n",
    "        st = torch.load(stage2_w, map_location=\"cpu\")\n",
    "        if isinstance(st, dict) and \"state_dict\" in st: st = st[\"state_dict\"]\n",
    "        self.stage2_net.load_state_dict(st, strict=True)\n",
    "\n",
    "    def run_stage0(self, img_bgr):\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        img_for_model = change_color(img_rgb)\n",
    "        batch = s0c.image_to_batch(img_for_model)\n",
    "        with torch.no_grad(), torch.amp.autocast(self.device.split(\":\")[0], dtype=torch.float32):\n",
    "            output = self.stage0_net(batch)\n",
    "        rotated, keypoint = s0c.output_to_predict(img_rgb, batch, output)\n",
    "        normalised, _, _ = s0c.normalise_by_homography(rotated, keypoint)\n",
    "        return normalised\n",
    "\n",
    "    def run_stage1(self, stage0_img_rgb):\n",
    "        image = stage0_img_rgb\n",
    "        batch = {'image': torch.from_numpy(np.ascontiguousarray(image.transpose(2, 0, 1))).unsqueeze(0)}\n",
    "        with torch.no_grad(), torch.amp.autocast(self.device.split(\":\")[0], dtype=torch.float32):\n",
    "            output = self.stage1_net(batch)\n",
    "        gridpoint_xy, _ = s1c.output_to_predict(image, batch, output)\n",
    "        return s1c.rectify_image(image, gridpoint_xy)\n",
    "\n",
    "    def run_stage2(self, stage1_img_rgb, length):\n",
    "        img = stage1_img_rgb[self.y0:self.y1, self.x0:self.x1] / 255.0\n",
    "        batch = self.resize(torch.from_numpy(np.ascontiguousarray(img.transpose(2, 0, 1))).unsqueeze(0)).float().to(self.device)\n",
    "        with torch.no_grad(), torch.amp.autocast(self.device.split(\":\")[0], dtype=torch.float32):\n",
    "            output = self.stage2_net(batch)\n",
    "        pixel = torch.sigmoid(output).float().cpu().numpy()[0]\n",
    "        series_in_pixel = s2c.pixel_to_series(pixel[..., self.t0:self.t1], self.zero_mv, length)\n",
    "        # adaptive scale\n",
    "        scale = np.std(series_in_pixel, axis=1, keepdims=True) + 1e-6\n",
    "        series = (np.array(self.zero_mv).reshape(4, 1) - series_in_pixel) / scale\n",
    "\n",
    "        for i in range(4):\n",
    "            s = savgol_filter(series[i], window_length=7, polyorder=2)\n",
    "            s = savgol_filter(s[::-1], window_length=7, polyorder=2)[::-1]\n",
    "            series[i] = s\n",
    "        return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ea270-b307-43ea-9764-a63f77b604c7",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 11 â€” Classifier configuration (model settings)\n",
    "\n",
    "input image resolution, and checkpoint path for pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d49fc6-b666-4ee1-a061-fcfca166a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_MODEL_NAME=\"efficientnet_b2\"\n",
    "CLS_NUM_CLASSES=12\n",
    "CLS_RESOLUTION=256\n",
    "CLS_CKPT_PATH=\"/kaggle/input/physionet-image-multi-class-train/efficientnet_b2_full_train.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86572c9-fc75-4cf9-8179-3d1869556527",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 12 â€” Build classifier model and load weights\n",
    "\n",
    "loads trained weights, removes \"module.\" prefix if present,\n",
    "and prepares model in evaluation mode on the selected device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e6d6d-648a-40ac-83e6-7d08bf5a08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(device=\"cuda\"):\n",
    "    m = timm.create_model(CLS_MODEL_NAME, pretrained=False, num_classes=CLS_NUM_CLASSES)\n",
    "    st = torch.load(CLS_CKPT_PATH, map_location=\"cpu\")\n",
    "    if isinstance(st, dict) and \"state_dict\" in st: st = st[\"state_dict\"]\n",
    "    st2 = {k.replace(\"module.\",\"\"): v for k, v in st.items()}\n",
    "    m.load_state_dict(st2, strict=False)\n",
    "    return m.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a4cfe-b8d1-4a49-ad15-0316da7a6917",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 13 â€” Image preprocessing for classifier\n",
    "\n",
    "normalizes using ImageNet mean/std, and converts to PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9b1d2-7e78-4fb7-8fd1-f5068a492f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_preprocess_bgr(img_bgr):\n",
    "    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (CLS_RESOLUTION, CLS_RESOLUTION), interpolation=cv2.INTER_AREA).astype(np.float32)/255.0\n",
    "    mean = np.array([0.485,0.456,0.406], np.float32); std = np.array([0.229,0.224,0.225], np.float32)\n",
    "    img = (img - mean) / std\n",
    "    return torch.from_numpy(img).permute(2,0,1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c973d-7da2-4a3b-af7e-d7e3319ecb71",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 14 â€” Predict ECG source class\n",
    "\n",
    "predicted source ID as formatted 4-digit string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbc837-0625-4732-9209-9eec0adae43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_source_suffix(model, img_bgr, device=\"cuda\"):\n",
    "    x = cls_preprocess_bgr(img_bgr).to(device)\n",
    "    p = F.softmax(model(x), dim=1)[0]\n",
    "    cls = int(torch.argmax(p).item())\n",
    "    return f\"{cls+1:04d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf77473-75bd-4a69-9814-8fb2d3703f53",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 15 â€” Select best Stage-1 result using classifier guidance\n",
    "\n",
    "Runs Stage-1 twice (raw vs source-processed) and selects\n",
    "the better one based on quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc095256-988e-4a40-a4ff-affc01bbebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_stage1_with_source(pipeline, img_raw_bgr, pred_source_suffix, selector_margin=1.02):\n",
    "    img_pp = preprocess_by_source(img_raw_bgr.copy(), pred_source_suffix)\n",
    "    s1_raw = pipeline.run_stage1(pipeline.run_stage0(img_raw_bgr))\n",
    "    q_raw  = stage1_quality(s1_raw)\n",
    "    s1_pp  = pipeline.run_stage1(pipeline.run_stage0(img_pp))\n",
    "    q_pp   = stage1_quality(s1_pp)\n",
    "    return s1_pp if q_pp > q_raw * selector_margin else s1_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f3a04-af3c-4d83-b4af-875d9b28e51d",
   "metadata": {},
   "source": [
    "ðŸ”¹Segment 16 â€” Convert predicted signals into submission format\n",
    "\n",
    "formats into Kaggle submission structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90d0a5-5fde-4436-a172-daa88a3faa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission_from_pred(base_id: str, fs: int, sig_len: int, d_series: dict):\n",
    "    base_id = str(base_id); fs = int(fs); sig_len = int(sig_len)\n",
    "    n_short = int(np.floor(fs * 2.5))\n",
    "\n",
    "    def take_segment(y, n):\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "        if len(y) >= n: return y[:n]\n",
    "        if len(y) == 0: return np.zeros(n, np.float64)\n",
    "        return np.concatenate([y, np.full(n - len(y), y[-1], np.float64)])\n",
    "\n",
    "    rows = []\n",
    "    for lead in LEADS_ORDER:\n",
    "        y = np.asarray(d_series[lead], dtype=np.float64)\n",
    "        y = align_by_xcorr(y, sig_len if lead==\"II\" else n_short)\n",
    "        seg = take_segment(y, sig_len if lead==\"II\" else n_short)\n",
    "        rows.append(pd.DataFrame({\"id\":[f\"{base_id}_{i}_{lead}\" for i in range(len(seg))],\n",
    "                                  \"value\":seg.astype(np.float32)}))\n",
    "    return pd.concat(rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536db44f-14d6-4c29-a601-7340aadc2bf0",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 17 â€” Load test metadata\n",
    "\n",
    "Loads test CSV and sample submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010bd5f-809c-4035-9f53-2dec00b0c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR=\"/kaggle/input/physionet-ecg-image-digitization\"\n",
    "df_test = pd.read_csv(f\"{WORK_DIR}/test.csv\")\n",
    "df_test[\"id\"] = df_test[\"id\"].astype(str)\n",
    "sample_submission = pd.read_parquet(f\"{WORK_DIR}/sample_submission.parquet\")[[\"id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eaaa43-52d3-48a9-8b58-819564ee1cf1",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 18 â€” Initialize models and pipeline\n",
    "\n",
    "loads Stage-0, Stage-1, Stage-2 weights, and builds classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136f8d5-f7bf-4e18-81bf-43b662dc8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = PhysioPipeline(device=\"cuda:0\" if device==\"cuda\" else \"cpu\")\n",
    "pipeline.load_models(\n",
    "    stage0_w=\"/kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet/weight/stage0-last.checkpoint.pth\",\n",
    "    stage1_w=\"/kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet/weight/stage1-last.checkpoint.pth\",\n",
    "    stage2_w=\"/kaggle/input/physio-seg-public/pytorch/net3_009_4200/1/iter_0004200.pt\",\n",
    ")\n",
    "cls_model = build_classifier(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe73a2c-6e1a-4fe0-8e23-9dbac012145c",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 19 â€” Main inference loop\n",
    "\n",
    "read image, predict source, choose best Stage-1 output,\n",
    "run Stage-2 to extract signals, and prepare submission rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069c431-6887-4880-8a2e-86b829a362d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for sample_id, g in df_test.groupby(\"id\", sort=True):\n",
    "    img_path = f\"{WORK_DIR}/test/{sample_id}.png\"\n",
    "    img_raw = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    if img_raw is None:\n",
    "        raise FileNotFoundError(img_path)\n",
    "\n",
    "    fs = int(g.fs.iloc[0])\n",
    "    sig_len = int(g.loc[g.lead==\"II\",\"number_of_rows\"].iloc[0])\n",
    "\n",
    "    pred_src = predict_source_suffix(cls_model, img_raw, device=device)\n",
    "    s1 = select_stage1_with_source(pipeline, img_raw, pred_src, selector_margin=1.05)\n",
    "    series_4row = pipeline.run_stage2(s1, length=sig_len)\n",
    "\n",
    "    d = dw(series_dict(series_4row))         # âœ… now safe (uses II_short)\n",
    "    res.append(make_submission_from_pred(sample_id, fs, sig_len, d))\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce97d5f-5b7b-48d2-923c-896e1824f1e1",
   "metadata": {},
   "source": [
    "ðŸ”¹ Segment 20 â€” Final submission creation\n",
    "\n",
    "validate correctness, and save submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7047f0ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:42:32.065146Z",
     "iopub.status.busy": "2026-01-20T07:42:32.064887Z",
     "iopub.status.idle": "2026-01-20T07:43:52.414715Z",
     "shell.execute_reply": "2026-01-20T07:43:52.413803Z"
    },
    "papermill": {
     "duration": 80.356358,
     "end_time": "2026-01-20T07:43:52.417697",
     "exception": false,
     "start_time": "2026-01-20T07:42:32.061339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\r\n",
      "Uninstalling tensorflow-2.18.0:\r\n",
      "  Successfully uninstalled tensorflow-2.18.0\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 164ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mconnected-components-3d\u001b[0m\u001b[2m==3.26.1\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS_DIR: /kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet\n",
      "REF_PT: (9, 2)\n",
      "/kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet\n",
      "/kaggle/input/hengck23-submit-physionet/hengck23-submit-physionet\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "OK  submission.csv (75000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.concat(res, ignore_index=True)\n",
    "df_submission = df_submission.set_index(\"id\").reindex(sample_submission[\"id\"]).reset_index()\n",
    "\n",
    "assert df_submission[\"value\"].notna().all()\n",
    "assert (df_submission[\"id\"].values == sample_submission[\"id\"].values).all()\n",
    "\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"OK  submission.csv\", df_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44bf2c",
   "metadata": {
    "papermill": {
     "duration": 0.00176,
     "end_time": "2026-01-20T07:43:52.421784",
     "exception": false,
     "start_time": "2026-01-20T07:43:52.420024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14096757,
     "sourceId": 97984,
     "sourceType": "competition"
    },
    {
     "datasetId": 8747012,
     "sourceId": 13746387,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8620533,
     "sourceId": 13816899,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 272137252,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 528480,
     "modelInstanceId": 513841,
     "sourceId": 677607,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 88.215596,
   "end_time": "2026-01-20T07:43:55.713288",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-20T07:42:27.497692",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
